{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and add Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords = stopwords + ['p','https','http','www','href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all functions for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    \n",
    "    # Get only valid data.\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "    \n",
    "    # Remove all punctuations.\n",
    "    data['Articles'] = data['Articles'].str.replace('[^\\w\\s]', ' ')\n",
    "    \n",
    "    # Remove all numbers and inbetween numbers.\n",
    "    data['Articles'] = data['Articles'].str.replace('\\w*\\d\\w*', ' ')\n",
    "    \n",
    "    # Remove all new line characters\n",
    "    data['Articles'] = data['Articles'].str.replace('\\s+', ' ')\n",
    "    \n",
    "    # Lower all values.\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(x.lower() for x in x.split())) \n",
    "    \n",
    "    # Remove stop words.\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(x for x in x.split() if x not in stopwords))\n",
    "    \n",
    "    # Remove underscores if any - Added after EDA.    \n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(x.replace('_', ' ') for x in x.split()))\n",
    "    \n",
    "    # Remove junk words with lenght less than 2.\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(x for x in x.split() if len(x)>2))\n",
    "    \n",
    "    # Remove all words that occure less tha 5 times in full data provided for cleaning.\n",
    "    freq = pd.Series(' '.join(data['Articles']).split()).value_counts()\n",
    "    rare_words = list(freq.index[freq.values < 5])\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(x for x in x.split() if x not in rare_words))\n",
    "    \n",
    "    # Stemm and lemmatize the words.\n",
    "    st = LancasterStemmer()\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(st.stem(x) for x in x.split()))\n",
    "\n",
    "    lt = WordNetLemmatizer()\n",
    "    data['Articles'] = data['Articles'].apply(lambda x: ' '.join(lt.lemmatize(x) for x in x.split()))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data considering important POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with POS.\n",
    "# remove_tags = ['CC', 'DT', 'EX', 'FW', 'UH', 'IN', 'TO']\n",
    "include_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "def remove_postags(list_words):\n",
    "    pos_tags = pos_tag(list_words)\n",
    "    list_words_new = []        \n",
    "    for tags in pos_tags:\n",
    "#         if tags[1] not in remove_tags:\n",
    "        if tags[1] in include_tags:\n",
    "            list_words_new.append(tags[0])\n",
    "    return list_words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_processing(data):\n",
    "    data_processed = pd.DataFrame(columns=['URL','Articles'])\n",
    "    for i in range(len(data)):\n",
    "        paragraphs = data.iloc[i,1]\n",
    "        \n",
    "        list_words = nltk.word_tokenize(paragraphs)        \n",
    "        list_words = remove_postags(list_words)\n",
    "\n",
    "        data_processed = data_processed.append({'URL': data.iloc[i,0], \\\n",
    "                                        'Articles':' '.join(word for word in list_words if len(word) < 15)}, ignore_index=True)\n",
    "#         print('Completed: {}'.format(i))\n",
    "\n",
    "    return(data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define i/p o/p file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows to be processed: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.giveitlove.com/hilarious-things-you...</td>\n",
       "      <td>[&lt;p&gt;The sun, sand, and water — the beach is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.joinhoney.com/g-blog/the-secret-on...</td>\n",
       "      <td>[&lt;p&gt;Whether you shop online a lot or just occa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.pastfactory.com/history/incredible-...</td>\n",
       "      <td>[&lt;p&gt;Did you know that Elvis invited himself to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.tiebreaker.com/toughest-hockey-pla...</td>\n",
       "      <td>[&lt;p&gt;by&lt;/p&gt;, &lt;p&gt;Hockey is arguably the toughest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ca.cdn.hearing-aid-advice.com/signia_f...</td>\n",
       "      <td>[&lt;p class=\"steps-cta__heading\"&gt;Here's how you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  http://www.giveitlove.com/hilarious-things-you...   \n",
       "1  https://www.joinhoney.com/g-blog/the-secret-on...   \n",
       "2  http://www.pastfactory.com/history/incredible-...   \n",
       "3  https://www.tiebreaker.com/toughest-hockey-pla...   \n",
       "4  https://ca.cdn.hearing-aid-advice.com/signia_f...   \n",
       "\n",
       "                                            Articles  \n",
       "0  [<p>The sun, sand, and water — the beach is a ...  \n",
       "1  [<p>Whether you shop online a lot or just occa...  \n",
       "2  [<p>Did you know that Elvis invited himself to...  \n",
       "3  [<p>by</p>, <p>Hockey is arguably the toughest...  \n",
       "4  [<p class=\"steps-cta__heading\">Here's how you ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_file_name = 'test_spam_raw.csv'\n",
    "processed_file_name = 'test_spam_processed.csv'\n",
    "processed_file_name_pos = 'test_spam_processed_pos.csv'\n",
    "\n",
    "data = pd.read_csv(raw_file_name)\n",
    "print('No of rows to be processed: {}'.format(data.shape[0]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = text_preprocessing(data)\n",
    "data_processed_pos = pos_processing(data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length b4 pos processing: 199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'beach plac day good company beach lik peopl on ord beach without least on list thing peopl ahead see good beach sur bet find beach sur could get much ev least good sint look ev though lot saw nee com'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Length b4 pos processing: {}'.format(len(data_processed['Articles'][0])))\n",
    "data_processed['Articles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after pos processing: 172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'beach plac day good company beach lik peopl ord beach least list thing peopl ahead see good beach sur bet find beach sur get much ev least good sint look ev lot saw nee com'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Length after pos processing: {}'.format(len(data_processed_pos['Articles'][0])))\n",
    "data_processed_pos['Articles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed.to_csv(processed_file_name, index=False)\n",
    "data_processed_pos.to_csv(processed_file_name_pos, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
